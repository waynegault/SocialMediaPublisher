# =============================================================================
# Social Media Publisher - Example Environment Configuration
# Copy to .env and fill in your values.
# =============================================================================

# --- API KEYS ---
GEMINI_API_KEY=your_gemini_key
OPENAI_API_KEY=your_openai_key
HUGGINGFACE_API_KEY=your_huggingface_key
# Skip HuggingFace (token lacks Inference permissions), use Imagen instead
HF_PREFER_IF_CONFIGURED=False

# LinkedIn Credentials (developer.linkedin.com)
linkedin_username=
linkedin_password=
LINKEDIN_AUTHOR_URN=
LINKEDIN_AUTHOR_NAME=
LINKEDIN_ACCESS_TOKEN=
LINKEDIN_API_VERSION=202501
# OAuth 2.0 app credentials (for programmatic token refresh)
LINKEDIN_CLIENT_ID=
LINKEDIN_CLIENT_SECRET=
LINKEDIN_REFRESH_TOKEN=
# Optional LinkedIn variables (add when needed):
# RAPIDAPI_KEY=
# LINKEDIN_ORGANIZATION_URN=
# LINKEDIN_LI_AT=
# LINKEDIN_JSESSIONID=

# --- LINKEDIN SEARCH & CONNECTION MASTER SWITCH ---
# WARNING: LinkedIn actively detects automation and may restrict or ban accounts.
# Set to true ONLY if you accept the risk of account restrictions.
# When false (default), all LinkedIn profile searching and connection requests are disabled.
LINKEDIN_SEARCH_ENABLED=false

# --- PROFESSIONAL DISCIPLINE ---
# Your professional discipline (e.g., "chemical engineer", "software developer", "data scientist")
# This is used in prompts and content generation to personalize the output
DISCIPLINE=chemical engineer

# --- LOCAL LLM (LM Studio) ---
# Set to True to use a local LLM via LM Studio instead of cloud APIs
PREFER_LOCAL_LLM=False
LM_STUDIO_BASE_URL=http://localhost:1234/v1
LM_STUDIO_MODEL=local-model

# --- GROQ API (Free Cloud LLM) ---
# Get free API key from: https://console.groq.com/keys
# Free tier: 30 requests/min, 14,400 requests/day
# When configured and PREFER_GROQ=True, Groq is used instead of Gemini for text generation
GROQ_API_KEY=
GROQ_MODEL=llama-3.3-70b-versatile
PREFER_GROQ=False

# --- SEARCH SETTINGS ---
# Story search provider: "gemini" (Gemini AI + Google Search grounding) or
#   "duckduckgo" (DuckDuckGo web search + Groq/Local LLM processing, free)
SEARCH_PROVIDER=gemini
# Search engine for LinkedIn profile lookups (google, bing, duckduckgo)
# DuckDuckGo is recommended - privacy-friendly and less likely to block
SEARCH_ENGINE=duckduckgo
# Browser automation backend: "uc" (undetected-chromedriver) or "nodriver" (recommended)
# nodriver is the successor to undetected-chromedriver with better anti-detection
BROWSER_BACKEND=nodriver
SEARCH_LOOKBACK_DAYS=7
# If TRUE, ignores lookback days and uses the timestamp of the last successful run
USE_LAST_CHECKED_DATE=True
# How often to run search cycles (in hours)
SEARCH_CYCLE_HOURS=24
# Maximum number of stories to request per search
MAX_STORIES_PER_SEARCH=1
# Maximum people to search for LinkedIn profiles per story (reduces API calls)
MAX_PEOPLE_PER_STORY=3

# --- CONTENT SETTINGS ---
# Target word count for story summaries (longer = more detailed posts)
SUMMARY_WORD_COUNT=200
# Minimum quality score to generate images (1-10, higher = stricter)
MIN_QUALITY_SCORE=8

# --- DEDUPLICATION SETTINGS ---
# Jaccard similarity threshold for semantic deduplication (0.0 to 1.0)
# Higher values = stricter matching (fewer false duplicates)
DEDUP_SIMILARITY_THRESHOLD=0.7

# --- LINKEDIN PROFILE REQUIREMENT ---
# When True, stories must have identified people with LinkedIn profiles to pass verification
# When False, stories can pass verification based on content quality alone
REQUIRE_LINKEDIN_PROFILES=False
# Minimum profiles needed when REQUIRE_LINKEDIN_PROFILES=True (default: 1)
MIN_LINKEDIN_PROFILES=1

# --- URL VALIDATION SETTINGS ---
# Enable URL validation before saving stories
VALIDATE_SOURCE_URLS=True

# --- API RETRY SETTINGS ---
# Number of retry attempts for transient API failures
API_RETRY_COUNT=3
# Base delay in seconds between retries (doubles each attempt)
API_RETRY_DELAY=1.0

# --- PUBLICATION SETTINGS ---
# Maximum stories to publish per day
MAX_STORIES_PER_DAY=3
# Publishing window start/end times (HH:MM, 24-hour format)
START_PUB_TIME=08:00
END_PUB_TIME=20:00
# Allow posting time to vary by +/- this many minutes
JITTER_MINUTES=30
# How often to check for due publications (in seconds)
PUBLISHER_CHECK_INTERVAL_SECONDS=600

# --- CLEANUP SETTINGS ---
# Clean up old unused stories after this many days
EXCLUSION_PERIOD_DAYS=30

# --- Promotion Message ---
# Details to be included in promotion messages
SIGNATURE_BLOCK_DETAIL=MEng Chemical and Process Engineering

# --- STORAGE SETTINGS ---
# Database file name
DB_NAME=content_engine.db
# Directory for generated images
IMAGE_DIR=generated_images

# --- IMAGE GENERATION SETTINGS ---
# Control human presence in generated images
# YES/True = Include central human character
# NO/False = No central character; random humans only if incidental/peripheral
HUMAN_IN_IMAGE=False

# --- EXTENSIBLE IMAGE PROVIDER (switch providers via .env) ---
# Supported providers: cloudflare, ai_horde, pollinations, huggingface
# Default is 'pollinations' - completely free, no API key required
IMAGE_PROVIDER=pollinations
IMAGE_MODEL=flux-realism
IMAGE_SIZE=1024x1024
IMAGE_TIMEOUT_SECONDS=120

# --- FREE PROVIDERS (no API key required) ---

# Pollinations.AI - completely free, no signup (default)
# Models: flux-realism, flux, turbo

# AI Horde (community-run, truly free, queue-based - may be slower)
# Models: SDXL 1.0, Deliberate, Anything
# AI_HORDE_API_KEY=0000000000

# HuggingFace - free tier (may have quota limits)
# Models: black-forest-labs/FLUX.1-schnell
# HUGGINGFACE_API_TOKEN=optional

# Cloudflare Workers AI (generous free tier ~10k images/day)
# CLOUDFLARE_ACCOUNT_ID=your_account_id
# CLOUDFLARE_API_TOKEN=your_api_token

# RapidAPI Key for Fresh LinkedIn Profile Data API
# Subscribe at: https://rapidapi.com/freshdata-freshdata-default/api/fresh-linkedin-profile-data/pricing
RAPIDAPI_KEY=

# =============================================================================
# Z-Image Local Image Generation (Optional)
# =============================================================================
# Requires NVIDIA GPU with CUDA support (4GB+ VRAM with CPU offload)
# Install with: pip install diffusers accelerate sentencepiece
#
# To use Z-Image, set IMAGE_PROVIDER=z_image and configure below.
#
# Z_IMAGE_STEPS: Inference steps (12-50, lower=faster, default: 28)
# Z_IMAGE_GUIDANCE: CFG guidance scale (3.0-5.0, default: 4.0)
# Z_IMAGE_DEVICE: Device ('cuda' or 'cpu', default: cuda)
# Z_IMAGE_OFFLOAD: CPU offload strategy (default: none)
#   - 'sequential': Best for <=4.5GB VRAM (loads one layer at a time)
#   - 'model': Good for 4.5-8GB VRAM (moves whole model CPU<->GPU)
#   - 'none' or '0': For 12GB+ VRAM (keeps everything on GPU)
# Z_IMAGE_DTYPE: Data type (default: bfloat16)
#   - 'float16': Fastest on most GPUs
#   - 'bfloat16': Native on Ampere+, may be required by some models
# Z_IMAGE_SIZE: Default image size (e.g., '768x512', default: 1024x1024)
# Z_IMAGE_NEGATIVE_PROMPT: Default negative prompt
#
# Tested optimal for RTX 3050 Ti (4GB VRAM):
#   Z_IMAGE_STEPS=20
#   Z_IMAGE_GUIDANCE=4.0
#   Z_IMAGE_OFFLOAD=sequential
#   Z_IMAGE_DTYPE=bfloat16
#   Z_IMAGE_SIZE=512x512
# =============================================================================
